---
title: "Mobile phone price classifiaction"
author: "Stephen Njuguna"
date: "2/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Mobile Phone Price Classification
    
## Business Understanding

The increasing use of smartphones is evident in the world today. For mobile dealers, an effective pricing strategy is essential for continued sales success. It is important to have a carefully planned pricing strategy.

Smartphone prices can be affected by a wide range of factors, such as brand and the length of time a particular device has been on the market. The ability to use a device internationally can also affect its price. If a device has a high-resolution camera and a lot of storage capacity, it will likely cost more than a unit without less notable capabilities. Those individuals who are willing to enter a contract, however, may be able to get a smartphone loaded with desirable features at a bargain price.

### Defining the question

Basing on the phones features we build a machine learning model that predicts  its price. 


### Defining the metrics for metrics for Success

Build a machine learning model that predicts the price class of a mobile phone.

### Experimental Design 

The following approach will be taken to achieve our goal:
1) Loading the data

2) Data cleaning

3) Exploratory Data analysis

4) Feature Engineering

5) Data modelling using Supervised learning algorithms

6) Challenging the solution Using unsupervised learning.

7) Conclusions and reccomendations.


## Reading data

```{r}
#Loading the libraries 
library(data.table)
library(tidyverse)

```

```{r}
#Reading the data set
phone_df <- read.csv("C:/Users/Njuguna/Downloads/Data_train.csv", header = TRUE) 

#Previewing the top 6 entries
head(phone_df)

```

```{r}
#Checking for missing values

colSums(is.na(phone_df))

```

The dataset has no null values.

```{r}

#Checking for duplicates

duplicate <- phone_df[duplicated(phone_df),]
duplicate

```

The dataset has no duplicates 

### Checking For outliers
```{r}
# Splitting the numerical  variables from the categorical variables
num_columns <- c("battery_power", "clock_speed", "fc", "int_memory", "m_dep", "mobile_wt","n_cores", "pc", "px_height", "px_width", "ram", "sc_h", "sc_w", "talk_time")

num_df<- phone_df[,num_columns]
#Previewing the numerical columns
head(num_df)

# Getting the categorical variables
cat_df <- phone_df[c(2,4,6,18,19,20,21)]
head(cat_df)

```


### Anomaly Detection


```{r}
summary(num_df)
```
From the summary we note that there are no anomalies that can be detected at this point.

## EXPLORATORY DATA ANALYSIS

UNIVARIATE ANALYSIS

```{r}
#Converting the encoded columns back to names
phone_df$price_range <- factor(phone_df$price_range, levels = c(0,1,2,3),
                       labels = c("low cost", "medium cost", "high cost", "very high cost"))

phone_df$blue <- factor(phone_df$blue, levels = c(0,1),
                                  labels = c("not", "yes"))
phone_df$dual_sim <- factor(phone_df$dual_sim, levels = c(0,1),
                               labels = c("not", "yes"))
phone_df$four_g <- factor(phone_df$four_g, levels = c(0,1),
                               labels = c("not", "yes"))
phone_df$three_g <- factor(phone_df$three_g, levels = c(0,1),
                               labels = c("not", "yes"))
phone_df$touch_screen <- factor(phone_df$touch_screen, levels = c(0,1),
                               labels = c("not", "yes"))
phone_df$wifi <- factor(phone_df$wifi, levels = c(0,1),
                               labels = c("not", "yes"))
```


```{r}

#plotting price range 

library(ggplot2)
ggplot(data=phone_df, aes(x = price_range)) +
  geom_bar()

```


Dataset has the same amount in each class that consist of low cost, medium cost, high cost, and very high cost.


 
BIVARIATE DATA ANALYSIS


```{r}
#Plotting four_G and price range 

ggplot(phone_df, aes(x=price_range, fill = four_g)) +
  geom_bar(position = "dodge")

```


From the phones on the dataset, we note that there is a difference in the four_g phones with a very high cost. Most phones on that range are 4_gs.

```{r}
#Plotting three_G and price range 

ggplot(phone_df, aes(x=price_range, fill = three_g)) +
  geom_bar(position = "dodge")

```


There is a very low amount of phones that are not 3_G enabled accross all price ranges.


```{r}
#Plotting dual sim and price range 
library(ggplot2)

ggplot(phone_df, aes(x=price_range, fill = dual_sim)) +
  geom_bar(position = "dodge")

```
  
  
  * low cost mobile are bought equally with without dual sim.
  * Medium cost mobiles with dual sim are bought more than without.
  * High cost mobile are bought equally with without dual sim.
  * Very high cost mobile with dula sim are bought more that those without.
  
```{r}

#Ploting mobiles with wifi aganist price range 
ggplot(phone_df, aes(x=price_range, fill = wifi)) +
  geom_bar(position = "dodge")

```
  
  
  * low cost mobile with wifi have a small diffence on how they are bought.
  * Medium cost mobiles with wifi have a small diffence on how they are bought.
  * High cost mobile with wifi have a small diffence on how they are bought.
  * Very high cost mobile with with wife are bought more that those without.
  
```{r}

#Ploting mobiles that are touch screen aganist price range 
ggplot(phone_df, aes(x=price_range, fill = touch_screen)) +
  geom_bar(position = "dodge")

```
  
 
  * low cost mobiles that are touch screened are bought more than those that not touch screened.
  * medium cost mobiles that are touch screened are bought more than those that not touch            screened.
  * high cost mobiles that are not touch screened are bought more than those that are touch         screened.
  * Very high cost mobiles that are touch screened are bought more than those that are not touch    screened.
  
  
```{r}

#Ploting mobiles that are three_g aganist price range 

ggplot(phone_df, aes(x=price_range, fill = three_g)) +
  geom_bar(position = "dodge")

```

 
  
  * Mobiles that are three_g are bought less than though that are not three_g across
  all price ranges.
  
  
```{r}
#plotting battery power by price range

ggplot(phone_df, aes(x=price_range, y=battery_power))+ geom_boxplot() + labs(title = "Battery power distribution by Price range") 

```


From the boxplots above we note that as the battery capacity increases the price range of the phone increases.



```{r}
#Plotting Clock speed distribution by Price range

ggplot(phone_df, aes(x=price_range, y=clock_speed))+ geom_boxplot() + labs(title = "Clock speed distribution by Price range") 

```


We also note that the clock speed does not affect the price of the phones.



```{r}

#Plotting a scatter plot between battery power and clock speed 

plot(phone_df$battery_power, phone_df$clock_speed, xlab="Battery power", ylab="clock speed")

```


There is no correlation battery power and clock speed.



```{r}

# Plotting scatter plot of phone weight and battery power

plot(num_df$battery_power, num_df$mobile_wt, xlab="Battery power", ylab="Phone weight")

```


Battery power and phone weight have no correlation

```{r}

# Plotting scatter plot of RAM and internal memory

plot(num_df$ram, num_df$int_memory, xlab="ram", ylab="Internal Memory")

```


There is no correlation between ram and internal memory

```{r}

#Finding the correlation of the numerical columns

library("dplyr") 
library(corrplot)


#Assigning m to the correlation
# correlation matrix
M<-cor(num_df)

corrplot(M, order = "hclust")


```

* There is a high positive correlation front camera megapixels and primary camera megapixels
* there is a moderately positive correlation between phone height and phone width



## FEATURE ENGINEERING

Label encoding columns

```{r}

# Import the library for label encoding

library(superml)

```

```{r}

# Introduce the label encoder object
label <- LabelEncoder$new()

#Label encoding the columns
phone_df$price_range <-label$fit_transform(phone_df$price_range)
phone_df$blue <-label$fit_transform(phone_df$blue)
phone_df$dual_sim <-label$fit_transform(phone_df$dual_sim)
phone_df$four_g <-label$fit_transform(phone_df$four_g)
phone_df$three_g <-label$fit_transform(phone_df$three_g)
phone_df$touch_screen <-label$fit_transform(phone_df$touch_screen)
phone_df$wifi <-label$fit_transform(phone_df$wifi)


```

Feature Selection

```{r}
#install.packages('Boruta')

library(Boruta)
```


```{r}
#Feature selection using 'Boruta' which works like Random forest 

boruta_output <- Boruta(price_range~ ., data=phone_df, doTrace=0)
names(boruta_output)

```


```{r}

# Get significant variables including tentatives

boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif) 

```

### LINEAR DISCRIMINANT ANALYSIS

Since LDA assumes a normal distribution we first normalize the data.

```{r}

#define Min-Max normalization function

min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
  }

#apply Min-Max normalization to numerical columns
phone_norm <- as.data.frame(lapply(num_df, min_max_norm))

```


```{r}

#add the target column to our normalized dataset

phone_norm$price_range <- phone_df$price_range

#view first six rows of data
head(phone_norm)

```

```{r}

# Importing the necessary libraries
  
library(MASS)
library(tidyverse)
library(caret)
theme_set(theme_classic())
  
  
# Split the data into training (80%) and test set (20%)
set.seed(123)
training_data <- phone_norm$price_range %>% 
            createDataPartition(p = 0.8, list = FALSE)
train_data <- phone_norm[training_data, ]
test_data <- phone_norm[-training_data, ]

head(train_data)
head(test_data)

```


```{r}

#Scaling the data
prep <- train_data %>% 
  preProcess(method = c("center", "scale"))
  
# Transform the data using the estimated parameters
train_transform <- prep %>% predict(train_data)
test_transform <- prep %>% predict(test_data)
  
# Fit the model
lda_model <- lda(price_range~., data = train_transform)
  
# Make predictions
lda_pred <- lda_model %>% predict(test_transform)

```

```{r}
# Model accuracy
mean(lda_pred$class==test_transform$price_range)
  
lda_model <- lda(price_range~., data = train_transform)
lda_model

```


```{r}
#Selecting the important features

imp_features <- phone_df[c('battery_power', 'px_height', 'px_width', 'sc_h', 'sc_w', 'ram')]
head(imp_features)

```


```{r}

#Combining the important features with the target columns
df <- do.call(cbind,list(imp_features, phone_df['price_range']))
head(df)

```


## SUPERVISED LEARNING

1) LOGISTIC REGRESSION


```{r}
# Loading package

library(caTools)
library(ROCR) 
library(nnet)  


# Splitting dataset into 80% train with 20% test

split <- sample.split(df, SplitRatio = 0.8)
split

#Creating a test and train set. 
train_log <- subset(df, split == "TRUE")
test_log <- subset(df, split == "FALSE")

#Previewing the top 6 entries of both train and test
head(train_log)
head(test_log)

```


```{r}

# Fit the multinomial class to the dataset
model <- nnet::multinom(price_range ~., data = train_log)
# Summarize the model
summary(model)
# Make predictions
predicted.classes <- model %>% predict(test_log)
head(predicted.classes)
# Model accuracy
mean(predicted.classes == test_log$price_range)

```


```{r}

#Finding the accuracy of the model.
mean(predicted.classes == test_log$price_range)

```


```{r}
# Previewing the Confusion Matrix

log_cm = table(test_log$`price_range`, predicted.classes)
confusionMatrix(log_cm)

```
From the metric analysis we note that we have 93.87% accuracy with the confusion matrix showing a few wrongly classified cases.


2) K-NEAREST NEIGHBOUR

```{r}
# Loading package

library(e1071)
library(class)
library(caTools)
library(ROCR)   
  
# Splitting data into train
# and test data
split <- sample.split(df, SplitRatio = 0.7)
train_knn <- subset(df, split == "TRUE")
test_knn <- subset(df, split == "FALSE")
  
# Feature Scaling
train_scale <- scale(train_knn[,c(1:6)])
test_scale <- scale(test_knn[,c(1:6)])
  
# Fitting KNN Model 
# to training dataset
classifier_knn <- knn(train = train_knn,
                      test = test_knn,
                      cl = train_knn$price_range,
                      k = 44)
```

Using a random number of k at 44 we check its accuracy.


```{r}
# Previewing the Confusion Matrix
cm = table(test_knn$`price_range`, classifier_knn)
confusionMatrix(cm)

```
Using K at 44 we find a accuracy of 92.8%. We then try and find the optimum number of k


#### Finding optimum K

```{r}

library(ISLR)
library(caret)
# Split the data:
indxTrain <- createDataPartition(y = df$price_range,p = 0.75,list = FALSE)
training <- df[indxTrain,]
testing <- df[-indxTrain,]
# Run k-NN:
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3)
knnFit <- train(price_range ~ ., data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"),tuneLength = 20)
knnFit

```
The optimum number of neighbor is 13.


```{r}

# Fitting KNN Model with optimal k_neighbor
# and training the model
classifier_knn2 <- knn(train = train_knn,
                      test = test_knn,
                      cl = train_knn$price_range,
                      k = 13)
```


```{r}
# Previewing the Confusion Matrix
cm2 = table(test_knn$`price_range`, classifier_knn2)
confusionMatrix(cm2)

```
After optimization the accuracy slightly increases to 93.3


3) Support Vector Machine

```{r}

# Splitting the dataset into the train set and test set

library(caTools)
 
set.seed(123)
split = sample.split(df$price_range, SplitRatio = 0.75)
 
train_set = subset(df, split == TRUE)
test_set = subset(df, split == FALSE)

head(train_set)
head(test_set)

```


```{r}

# Feature Scaling 
train_set[-7] <- scale(train_set[-7])
test_set[-7] <- scale(test_set[-7])

```


```{r}

# Fit and train the model 
library(e1071)
 
svm_classifier = svm(formula = price_range ~ .,
                 data = train_set,
                 type = 'C-classification',
                 kernel = 'linear')

```


```{r}

# Predicting the Test set results

y_pred = predict(svm_classifier, newdata = test_set[-7])

```



```{r}

#Evaluating the model 
# Previewing the Confusion Matrix

cm_svm = table(test_set[, 7], y_pred)
confusionMatrix(cm_svm)

```
Model accuracy using different kernels:

  * Polynomial kernel give a accuracy of 72%
  * Sigmoid kernel gives an accuracy of 89%
  * linear kernel gives accuracy of 96%
  * Radial basis kernel gives an accuracy of 86%





## Conclusion

  * High-cost mobile without for_g  are bought more than those with four_g.
  * Very high-cost mobile phones with four_g are bought more than those without.
  * High-cost mobiles that are not touch screen are bought more than those that are touch  screen
  * Very high-cost mobiles that are touch screened are bought more than those that are not touch            screened.
  * High-cost mobile phones are bought equally without a dual sim.
  * Very high-cost mobile phones with dual sims are bought more than those without.
  * High-cost mobile phones with wifi have a small difference in how they are bought.
  * Very high-cost mobile phones with wifi are bought more than those without.
  * Very high-cost mobile phones with battery power are bought more than those without.
  * There is no big difference in the clock speed in the different mobile price range
  * Very high-cost mobile phones with battery power are bought more than those without.


## Recommendations 
  * In order to make more profits;

  * The phone reseller should stock more phones of very high cost with four_g   because they are bought     more than the other phones.
  * The phone reseller should stock more phones that are of very high cost and are touch screened are       because they bought more than those that are not touch screened.
  * The phone reseller should stock more phones that are of very high cost and with wifi are because        they are bought more than those without.
  *The phone reseller should stock more phones that are of very high cost and with dual sim is because      they bought more than those without.
  * The phone reseller should stock more phones that are of low-cost mobiles that are touch screen          because they are bought more than those that are not touch screened.


The most important features in determining the price range were;
Battery_power- Total energy a battery can store in one time measured in mAh
RAM- Random Acess Memory in gigabites
Px_height - Pixel Resolution Height
Px_width - Pixel Resolution Width
Sc_h - Screen Height of mobile in cm
Sc_w - Screen Width of mobile in cm

From the supervised model, the best model for predicting the mobile price class range is the Support Vector Machines model with an accuracy of 96% which was better than the other models used to make predictions.
